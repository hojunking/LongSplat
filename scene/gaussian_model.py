# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.
#
# NVIDIA CORPORATION and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.  Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA CORPORATION is strictly prohibited.

import torch
from functools import reduce
import numpy as np
from torch_scatter import scatter_max
from utils.general_utils import inverse_sigmoid, get_expon_lr_func
from torch import nn
import os
from utils.system_utils import mkdir_p
from plyfile import PlyData, PlyElement
from simple_knn._C import distCUDA2
from utils.graphics_utils import BasicPointCloud, geom_transform_points, depth_edge
from utils.general_utils import strip_symmetric, build_scaling_rotation
from scene.embedding import Embedding
from einops import repeat

class GaussianModel:

    def setup_functions(self):
        def build_covariance_from_scaling_rotation(scaling, scaling_modifier, rotation):
            L = build_scaling_rotation(scaling_modifier * scaling, rotation)
            actual_covariance = L @ L.transpose(1, 2)
            symm = strip_symmetric(actual_covariance)
            return symm
        
        self.scaling_activation = torch.exp
        self.scaling_inverse_activation = torch.log

        self.covariance_activation = build_covariance_from_scaling_rotation

        self.opacity_activation = torch.sigmoid
        self.inverse_opacity_activation = inverse_sigmoid

        self.rotation_activation = torch.nn.functional.normalize


    def __init__(self, 
                 feat_dim: int=32, 
                 n_offsets: int=5, 
                 voxel_size: float=0.01,
                 update_depth: int=3, 
                 update_init_factor: int=100,
                 update_hierachy_factor: int=4,
                 use_feat_bank : bool = False,
                 appearance_dim : int = 32,
                 ratio : int = 1,
                 add_opacity_dist : bool = False,
                 add_cov_dist : bool = False,
                 add_color_dist : bool = False,
                 ):
        self.active_sh_degree = 0
        self.max_sh_degree = 3 
        self.feat_dim = feat_dim
        self.n_offsets = n_offsets
        self.voxel_size = voxel_size
        self.update_depth = update_depth
        self.update_init_factor = update_init_factor
        self.update_hierachy_factor = update_hierachy_factor
        self.use_feat_bank = use_feat_bank

        self.appearance_dim = appearance_dim
        self.embedding_appearance = None
        self.ratio = ratio
        self.add_opacity_dist = add_opacity_dist
        self.add_cov_dist = add_cov_dist
        self.add_color_dist = add_color_dist

        self._anchor = torch.empty(0)
        self._offset = torch.empty(0)
        self._anchor_feat = torch.empty(0)
        
        self.opacity_accum = torch.empty(0)

        self._scaling = torch.empty(0)
        self._rotation = torch.empty(0)
        self._opacity = torch.empty(0)
        self.max_radii2D = torch.empty(0)
        
        self.offset_gradient_accum = torch.empty(0)
        self.offset_denom = torch.empty(0)

        self.anchor_demon = torch.empty(0)
                
        self.optimizer = None
        self.percent_dense = 0
        self.spatial_lr_scale = 0
        self.setup_functions()

        if self.use_feat_bank:
            self.mlp_feature_bank = nn.Sequential(
                nn.Linear(3+1, feat_dim),
                nn.ReLU(True),
                nn.Linear(feat_dim, 3),
                nn.Softmax(dim=1)
            ).cuda()

        self.opacity_dist_dim = 1 if self.add_opacity_dist else 0
        self.mlp_opacity = nn.Sequential(
            nn.Linear(feat_dim+3+self.opacity_dist_dim, feat_dim),
            nn.ReLU(True),
            nn.Linear(feat_dim, n_offsets),
            nn.Tanh()
        ).cuda()

        self.add_cov_dist = add_cov_dist
        self.cov_dist_dim = 1 if self.add_cov_dist else 0
        self.mlp_cov = nn.Sequential(
            nn.Linear(feat_dim+3+self.cov_dist_dim, feat_dim),
            nn.ReLU(True),
            nn.Linear(feat_dim, 7*self.n_offsets),
        ).cuda()

        self.color_dist_dim = 1 if self.add_color_dist else 0
        self.mlp_color = nn.Sequential(
            nn.Linear(feat_dim+3+self.color_dist_dim+self.appearance_dim, feat_dim),
            nn.ReLU(True),
            nn.Linear(feat_dim, 3*self.n_offsets),
            nn.Sigmoid()
        ).cuda()

        # 추가
        self._importance = torch.empty(0, device="cuda")


    def eval(self):
        self.mlp_opacity.eval()
        self.mlp_cov.eval()
        self.mlp_color.eval()
        if self.appearance_dim > 0:
            self.embedding_appearance.eval()
        if self.use_feat_bank:
            self.mlp_feature_bank.eval()

    def train(self):
        self.mlp_opacity.train()
        self.mlp_cov.train()
        self.mlp_color.train()
        if self.appearance_dim > 0:
            self.embedding_appearance.train()
        if self.use_feat_bank:                   
            self.mlp_feature_bank.train()

    def capture(self):
        return (
            self._anchor,
            self._offset,
            self._local,
            self._scaling,
            self._rotation,
            self._opacity,
            self.max_radii2D,
            self.denom,
            self.optimizer.state_dict(),
            self.spatial_lr_scale,
        )
    
    def restore(self, model_args, training_args):
        (self.active_sh_degree, 
        self._anchor, 
        self._offset,
        self._local,
        self._scaling, 
        self._rotation, 
        self._opacity,
        self.max_radii2D, 
        denom,
        opt_dict, 
        self.spatial_lr_scale) = model_args
        self.training_setup(training_args)
        self.denom = denom
        self.optimizer.load_state_dict(opt_dict)

    def set_appearance(self, num_cameras):
        if self.appearance_dim > 0:
            self.embedding_appearance = Embedding(num_cameras, self.appearance_dim).cuda()

    @property
    def get_appearance(self):
        return self.embedding_appearance

    @property
    def get_scaling(self):
        return 1.0*self.scaling_activation(self._scaling)
    
    @property
    def get_featurebank_mlp(self):
        return self.mlp_feature_bank
    
    @property
    def get_opacity_mlp(self):
        return self.mlp_opacity
    
    @property
    def get_cov_mlp(self):
        return self.mlp_cov

    @property
    def get_color_mlp(self):
        return self.mlp_color
    
    @property
    def get_rotation(self):
        return self.rotation_activation(self._rotation)
    
    @property
    def get_anchor(self):
        return self._anchor
    
    @property
    def set_anchor(self, new_anchor):
        assert self._anchor.shape == new_anchor.shape
        del self._anchor
        torch.cuda.empty_cache()
        self._anchor = new_anchor
    
    @property
    def get_opacity(self):
        return self.opacity_activation(self._opacity)

    @property
    def get_xyz(self):
        return self._xyz
    
    @property
    def get_features(self):
        features_dc = self._features_dc
        features_rest = self._features_rest
        return torch.cat((features_dc, features_rest), dim=1)
    
    @property
    def get_features_dc(self):
        return self._features_dc
    
    @property
    def get_features_rest(self):
        return self._features_rest
    
    def get_covariance(self, scaling_modifier = 1):
        return self.covariance_activation(self.get_scaling, scaling_modifier, self._rotation)

    def oneupSHdegree(self):
        if self.active_sh_degree < self.max_sh_degree:
            self.active_sh_degree += 1
    
    def voxelize_sample(self, data=None, voxel_size=0.01):
        data = data[torch.randperm(data.shape[0])]
        data = torch.unique(torch.round(data/voxel_size), dim=0)*voxel_size

        return data
    
    
    
    def octree_sample(self, data, levels=3, base_density_threshold=100):
        from collections import defaultdict
        def is_inside_voxel(point, voxel_center, voxel_size):
            half_size = voxel_size / 2
            lower_bound = voxel_center - half_size
            upper_bound = voxel_center + half_size

            return (lower_bound[0] <= point[0] < upper_bound[0] and
                    lower_bound[1] <= point[1] < upper_bound[1] and
                    lower_bound[2] <= point[2] < upper_bound[2])

        
        data = data[torch.randperm(data.shape[0])]
        quantized = torch.round(data / self.voxel_size)

        voxel_counts = defaultdict(int)
        for pt in quantized.cpu().tolist():
            voxel_counts[tuple(pt)] += 1

        voxel_coords = torch.tensor(list(voxel_counts.keys()), dtype=torch.float32, device=data.device) * self.voxel_size
        data = voxel_coords
        
        octree = set(tuple(pt.cpu().tolist()) for pt in data)
        new_points = []
        voxel_sizes = torch.ones(data.shape[0], device=data.device) * self.voxel_size
        
        current_voxel_size = self.voxel_size
        
        original_data = data.clone()

        for depth in range(levels - 1):
            subdivided_points = []
            new_voxel_counts = defaultdict(int)

            next_voxel_size = current_voxel_size / 2
            density_threshold = base_density_threshold * (1 / (current_voxel_size / self.voxel_size))

            to_remove = set()

            for pt in list(octree):
                pt = torch.tensor(pt, dtype=torch.float32, device=data.device)

                if voxel_counts.get(tuple(pt.cpu().tolist()), 0) < density_threshold:
                    to_remove.add(tuple(pt.cpu().tolist()))
                    continue

                offsets = torch.tensor([
                    [-0.5, -0.5, -0.5], [-0.5, -0.5,  0.5], [-0.5,  0.5, -0.5], [-0.5,  0.5,  0.5],
                    [ 0.5, -0.5, -0.5], [ 0.5, -0.5,  0.5], [ 0.5,  0.5, -0.5], [ 0.5,  0.5,  0.5]
                ], dtype=torch.float32, device=data.device)

                sub_voxels = pt.unsqueeze(0) + offsets * next_voxel_size
                sub_voxels = torch.unique(sub_voxels, dim=0)

                valid_sub_voxels = []
                
                for sub in sub_voxels:
                    sub_tuple = tuple(sub.cpu().tolist())

                    count = 0
                    for original_point in original_data:
                        if is_inside_voxel(original_point, sub, next_voxel_size):
                            count += 1
                    
                    if count > 0:
                        valid_sub_voxels.append(sub)
                        octree.add(sub_tuple)
                        new_voxel_counts[sub_tuple] = count

                if valid_sub_voxels:
                    subdivided_points.append(torch.stack(valid_sub_voxels))
                    to_remove.add(tuple(pt.cpu().tolist()))

            for pt in to_remove:
                octree.discard(pt)

            voxel_counts = new_voxel_counts

            if len(subdivided_points) == 0:
                break

            new_points.append(torch.cat(subdivided_points, dim=0))
            voxel_sizes = torch.cat([voxel_sizes, torch.ones(new_points[-1].shape[0], device=data.device) * next_voxel_size], dim=0)
            current_voxel_size = next_voxel_size

        if len(new_points) > 0:
            new_points = torch.cat(new_points, dim=0)
            data = torch.cat([data, new_points], dim=0)

        return data, voxel_sizes

    def create_from_pcd(self, pcd : BasicPointCloud, spatial_lr_scale : float):
        self.spatial_lr_scale = spatial_lr_scale
        points = pcd.points[::self.ratio]

        if self.voxel_size <= 0:
            init_points = torch.tensor(points).float().cuda()
            init_dist = distCUDA2(init_points).float().cuda()
            median_dist, _ = torch.kthvalue(init_dist, int(init_dist.shape[0]*0.5))
            self.voxel_size = median_dist.item()
            del init_dist
            del init_points
            torch.cuda.empty_cache()

        print(f'Initial voxel_size: {self.voxel_size}')
        
        
        points, voxel_sizes = self.octree_sample(torch.tensor(points))
        fused_point_cloud = torch.tensor(np.asarray(points)).float().cuda()
        voxel_sizes = voxel_sizes.cuda()  # Move voxel_sizes to CUDA
        offsets = torch.zeros((fused_point_cloud.shape[0], self.n_offsets, 3)).float().cuda()
        anchors_feat = torch.zeros((fused_point_cloud.shape[0], self.feat_dim)).float().cuda()
        
        print("Number of points at initialisation : ", fused_point_cloud.shape[0])

        dist2 = torch.ones_like(fused_point_cloud[:, :1]) * voxel_sizes.unsqueeze(-1)
        scales = torch.log(torch.sqrt(dist2)).repeat(1, 6)
        
        rots = torch.zeros((fused_point_cloud.shape[0], 4), device="cuda")
        rots[:, 0] = 1

        opacities = inverse_sigmoid(0.1 * torch.ones((fused_point_cloud.shape[0], 1), dtype=torch.float, device="cuda"))

        self._anchor = nn.Parameter(fused_point_cloud.requires_grad_(True))
        self._offset = nn.Parameter(offsets.requires_grad_(True))
        self._anchor_feat = nn.Parameter(anchors_feat.requires_grad_(True))
        self._scaling = nn.Parameter(scales.requires_grad_(True))
        self._rotation = nn.Parameter(rots.requires_grad_(False))
        self._opacity = nn.Parameter(opacities.requires_grad_(False))
        self.max_radii2D = torch.zeros((self.get_anchor.shape[0]), device="cuda")


    def training_setup(self, training_args):
        self.percent_dense = training_args.percent_dense

        self.opacity_accum = torch.zeros((self.get_anchor.shape[0], 1), device="cuda")

        self.offset_gradient_accum = torch.zeros((self.get_anchor.shape[0]*self.n_offsets, 1), device="cuda")
        self.offset_denom = torch.zeros((self.get_anchor.shape[0]*self.n_offsets, 1), device="cuda")
        self.anchor_demon = torch.zeros((self.get_anchor.shape[0], 1), device="cuda")

        
        
        if self.use_feat_bank:
            l = [
                {'params': [self._anchor], 'lr': training_args.position_lr_init * self.spatial_lr_scale, "name": "anchor"},
                {'params': [self._offset], 'lr': training_args.offset_lr_init * self.spatial_lr_scale, "name": "offset"},
                {'params': [self._anchor_feat], 'lr': training_args.feature_lr, "name": "anchor_feat"},
                {'params': [self._opacity], 'lr': training_args.opacity_lr, "name": "opacity"},
                {'params': [self._scaling], 'lr': training_args.scaling_lr, "name": "scaling"},
                {'params': [self._rotation], 'lr': training_args.rotation_lr, "name": "rotation"},
                
                {'params': self.mlp_opacity.parameters(), 'lr': training_args.mlp_opacity_lr_init, "name": "mlp_opacity"},
                {'params': self.mlp_feature_bank.parameters(), 'lr': training_args.mlp_featurebank_lr_init, "name": "mlp_featurebank"},
                {'params': self.mlp_cov.parameters(), 'lr': training_args.mlp_cov_lr_init, "name": "mlp_cov"},
                {'params': self.mlp_color.parameters(), 'lr': training_args.mlp_color_lr_init, "name": "mlp_color"},
                {'params': self.embedding_appearance.parameters(), 'lr': training_args.appearance_lr_init, "name": "embedding_appearance"},
            ]
        elif self.appearance_dim > 0:
            l = [
                {'params': [self._anchor], 'lr': training_args.position_lr_init * self.spatial_lr_scale, "name": "anchor"},
                {'params': [self._offset], 'lr': training_args.offset_lr_init * self.spatial_lr_scale, "name": "offset"},
                {'params': [self._anchor_feat], 'lr': training_args.feature_lr, "name": "anchor_feat"},
                {'params': [self._opacity], 'lr': training_args.opacity_lr, "name": "opacity"},
                {'params': [self._scaling], 'lr': training_args.scaling_lr, "name": "scaling"},
                {'params': [self._rotation], 'lr': training_args.rotation_lr, "name": "rotation"},

                {'params': self.mlp_opacity.parameters(), 'lr': training_args.mlp_opacity_lr_init, "name": "mlp_opacity"},
                {'params': self.mlp_cov.parameters(), 'lr': training_args.mlp_cov_lr_init, "name": "mlp_cov"},
                {'params': self.mlp_color.parameters(), 'lr': training_args.mlp_color_lr_init, "name": "mlp_color"},
                {'params': self.embedding_appearance.parameters(), 'lr': training_args.appearance_lr_init, "name": "embedding_appearance"},
            ]
        else:
            l = [
                {'params': [self._anchor], 'lr': training_args.position_lr_init * self.spatial_lr_scale, "name": "anchor"},
                {'params': [self._offset], 'lr': training_args.offset_lr_init * self.spatial_lr_scale, "name": "offset"},
                {'params': [self._anchor_feat], 'lr': training_args.feature_lr, "name": "anchor_feat"},
                {'params': [self._opacity], 'lr': training_args.opacity_lr, "name": "opacity"},
                {'params': [self._scaling], 'lr': training_args.scaling_lr, "name": "scaling"},
                {'params': [self._rotation], 'lr': training_args.rotation_lr, "name": "rotation"},

                {'params': self.mlp_opacity.parameters(), 'lr': training_args.mlp_opacity_lr_init, "name": "mlp_opacity"},
                {'params': self.mlp_cov.parameters(), 'lr': training_args.mlp_cov_lr_init, "name": "mlp_cov"},
                {'params': self.mlp_color.parameters(), 'lr': training_args.mlp_color_lr_init, "name": "mlp_color"},
            ]

        self.optimizer = torch.optim.Adam(l, lr=0.0, eps=1e-15)
        self.anchor_scheduler_args = get_expon_lr_func(lr_init=training_args.position_lr_init*self.spatial_lr_scale,
                                                    lr_final=training_args.position_lr_final*self.spatial_lr_scale,
                                                    lr_delay_mult=training_args.position_lr_delay_mult,
                                                    max_steps=training_args.iterations)
        self.offset_scheduler_args = get_expon_lr_func(lr_init=training_args.offset_lr_init*self.spatial_lr_scale,
                                                    lr_final=training_args.offset_lr_final*self.spatial_lr_scale,
                                                    lr_delay_mult=training_args.offset_lr_delay_mult,
                                                    max_steps=training_args.iterations)
        
        self.mlp_opacity_scheduler_args = get_expon_lr_func(lr_init=training_args.mlp_opacity_lr_init,
                                                    lr_final=training_args.mlp_opacity_lr_final,
                                                    lr_delay_mult=training_args.mlp_opacity_lr_delay_mult,
                                                    max_steps=training_args.iterations)
        
        self.mlp_cov_scheduler_args = get_expon_lr_func(lr_init=training_args.mlp_cov_lr_init,
                                                    lr_final=training_args.mlp_cov_lr_final,
                                                    lr_delay_mult=training_args.mlp_cov_lr_delay_mult,
                                                    max_steps=training_args.iterations)
        
        self.mlp_color_scheduler_args = get_expon_lr_func(lr_init=training_args.mlp_color_lr_init,
                                                    lr_final=training_args.mlp_color_lr_final,
                                                    lr_delay_mult=training_args.mlp_color_lr_delay_mult,
                                                    max_steps=training_args.iterations)
        if self.use_feat_bank:
            self.mlp_featurebank_scheduler_args = get_expon_lr_func(lr_init=training_args.mlp_featurebank_lr_init,
                                                        lr_final=training_args.mlp_featurebank_lr_final,
                                                        lr_delay_mult=training_args.mlp_featurebank_lr_delay_mult,
                                                        max_steps=training_args.iterations)
        if self.appearance_dim > 0:
            self.appearance_scheduler_args = get_expon_lr_func(lr_init=training_args.appearance_lr_init,
                                                        lr_final=training_args.appearance_lr_final,
                                                        lr_delay_mult=training_args.appearance_lr_delay_mult,
                                                        max_steps=training_args.iterations)
        
        self.depth_loss_scheduler_args = get_expon_lr_func(lr_init=training_args.depth_l1_weight_init,
                                                        lr_final=training_args.depth_l1_weight_final,
                                                        max_steps=training_args.iterations)

        self.reproj_loss_scheduler_args = get_expon_lr_func(lr_init=training_args.reproj_loss_weight_init,
                                                        lr_final=training_args.reproj_loss_weight_final,
                                                        max_steps=training_args.iterations)
    
    def convert_to_3dgs(self, purning_mask=None):
        fused_point_cloud = repeat(self._anchor, 'n (c) -> (n k) (c)', k=self.n_offsets) + repeat(self.get_scaling[:, :3], 'n (c) -> (n k) (c)', k=self.n_offsets) * self._offset.view([-1, 3])

        if purning_mask is not None:
            fused_point_cloud = fused_point_cloud[purning_mask]

        fused_color = torch.zeros((fused_point_cloud.shape[0], 3)).float().cuda()
        features = torch.zeros((fused_color.shape[0], 3, (self.max_sh_degree + 1) ** 2)).float().cuda()
        features[:, :3, 0 ] = fused_color
        features[:, 3:, 1:] = 0.0

        print("Number of points at initialisation : ", fused_point_cloud.shape[0])

        dist2 = torch.clamp_max(distCUDA2(fused_point_cloud), self.voxel_size)
        scales = torch.log(torch.sqrt(dist2))[...,None].repeat(1, 3)
        rots = torch.zeros((fused_point_cloud.shape[0], 4), device="cuda")
        rots[:, 0] = 1

        opacities = self.inverse_opacity_activation(0.1 * torch.ones((fused_point_cloud.shape[0], 1), dtype=torch.float, device="cuda"))

        self._xyz = nn.Parameter(fused_point_cloud.requires_grad_(True))
        self._features_dc = nn.Parameter(features[:,:,0:1].transpose(1, 2).contiguous().requires_grad_(True))
        self._features_rest = nn.Parameter(features[:,:,1:].transpose(1, 2).contiguous().requires_grad_(True))
        self._scaling = nn.Parameter(scales.requires_grad_(True))
        self._rotation = nn.Parameter(rots.requires_grad_(True))
        self._opacity = nn.Parameter(opacities.requires_grad_(True))
        self.max_radii2D = torch.zeros((self.get_xyz.shape[0]), device="cuda")
    
    def training_setup_3dgs(self, training_args):
        self.percent_dense = training_args.percent_dense
        self.xyz_gradient_accum = torch.zeros((self.get_xyz.shape[0], 1), device="cuda")
        self.denom = torch.zeros((self.get_xyz.shape[0], 1), device="cuda")

        l = [
            {'params': [self._xyz], 'lr': training_args.position_lr_init * self.spatial_lr_scale, "name": "xyz"},
            {'params': [self._features_dc], 'lr': training_args.feature_lr, "name": "f_dc"},
            {'params': [self._features_rest], 'lr': training_args.feature_lr / 20.0, "name": "f_rest"},
            {'params': [self._opacity], 'lr': training_args.opacity_lr, "name": "opacity"},
            {'params': [self._scaling], 'lr': training_args.scaling_lr, "name": "scaling"},
            {'params': [self._rotation], 'lr': training_args.rotation_lr, "name": "rotation"}
        ]

        self.optimizer = torch.optim.Adam(l, lr=0.0, eps=1e-15)

        self.xyz_scheduler_args = get_expon_lr_func(lr_init=training_args.position_lr_init*self.spatial_lr_scale,
                                                    lr_final=training_args.position_lr_final*self.spatial_lr_scale,
                                                    lr_delay_mult=training_args.position_lr_delay_mult,
                                                    max_steps=training_args.position_lr_max_steps)

    def training_pose_setup(self, cameras, training_args):
        l = []
        for camera in cameras:
            l.append({'params': [camera.cam_rot_delta], 'lr': training_args.rotation_lr_init, "name": "cam_rot_{}".format(camera.uid)})
            l.append({'params': [camera.cam_trans_delta], 'lr': training_args.translation_lr_init, "name": "cam_trans_{}".format(camera.uid)})
        self.pose_optimizer = torch.optim.Adam(l)

        self.cam_rot_scheduler_args = get_expon_lr_func(lr_init=training_args.rotation_lr_init,
                                                    lr_final=training_args.rotation_lr_init * 0.1,
                                                    lr_delay_mult=0.01,
                                                    max_steps=training_args.iterations)
        
        self.cam_trans_scheduler_args = get_expon_lr_func(lr_init=training_args.translation_lr_init,
                                                    lr_final=training_args.translation_lr_init * 0.1,
                                                    lr_delay_mult=0.01,
                                                    max_steps=training_args.iterations)
        

    def update_learning_rate(self, iteration):
        ''' Learning rate scheduling per step '''
        for param_group in self.optimizer.param_groups:
            if param_group["name"] == "offset":
                lr = self.offset_scheduler_args(iteration)
                param_group['lr'] = lr
            if param_group["name"] == "anchor":
                lr = self.anchor_scheduler_args(iteration)
                param_group['lr'] = lr
            if param_group["name"] == "mlp_opacity":
                lr = self.mlp_opacity_scheduler_args(iteration)
                param_group['lr'] = lr
            if param_group["name"] == "mlp_cov":
                lr = self.mlp_cov_scheduler_args(iteration)
                param_group['lr'] = lr
            if param_group["name"] == "mlp_color":
                lr = self.mlp_color_scheduler_args(iteration)
                param_group['lr'] = lr
            if self.use_feat_bank and param_group["name"] == "mlp_featurebank":
                lr = self.mlp_featurebank_scheduler_args(iteration)
                param_group['lr'] = lr
            if self.appearance_dim > 0 and param_group["name"] == "embedding_appearance":
                lr = self.appearance_scheduler_args(iteration)
                param_group['lr'] = lr

    def update_pose_learning_rate(self, iteration):
        ''' Learning rate scheduling per step '''
        for param_group in self.pose_optimizer.param_groups:
            if param_group["name"].startswith("cam_rot"):
                lr = self.cam_rot_scheduler_args(iteration)
                param_group['lr'] = lr
            if param_group["name"].startswith("cam_trans"):
                lr = self.cam_trans_scheduler_args(iteration)
                param_group['lr'] = lr
            
            
    def construct_list_of_attributes(self):
        l = ['x', 'y', 'z', 'nx', 'ny', 'nz']
        for i in range(self._offset.shape[1]*self._offset.shape[2]):
            l.append('f_offset_{}'.format(i))
        for i in range(self._anchor_feat.shape[1]):
            l.append('f_anchor_feat_{}'.format(i))
        l.append('opacity')
        for i in range(self._scaling.shape[1]):
            l.append('scale_{}'.format(i))
        for i in range(self._rotation.shape[1]):
            l.append('rot_{}'.format(i))
        return l

    def construct_list_of_attributes_3dgs(self):
        l = ['x', 'y', 'z', 'nx', 'ny', 'nz']
        # All channels except the 3 DC
        for i in range(self._features_dc.shape[1]*self._features_dc.shape[2]):
            l.append('f_dc_{}'.format(i))
        for i in range(self._features_rest.shape[1]*self._features_rest.shape[2]):
            l.append('f_rest_{}'.format(i))
        l.append('opacity')
        for i in range(self._scaling.shape[1]):
            l.append('scale_{}'.format(i))
        for i in range(self._rotation.shape[1]):
            l.append('rot_{}'.format(i))
        return l

    def save_ply(self, path):
        mkdir_p(os.path.dirname(path))

        anchor = self._anchor.detach().cpu().numpy()
        normals = np.zeros_like(anchor)
        anchor_feat = self._anchor_feat.detach().cpu().numpy()
        offset = self._offset.detach().transpose(1, 2).flatten(start_dim=1).contiguous().cpu().numpy()
        opacities = self._opacity.detach().cpu().numpy()
        scale = self._scaling.detach().cpu().numpy()
        rotation = self._rotation.detach().cpu().numpy()

        dtype_full = [(attribute, 'f4') for attribute in self.construct_list_of_attributes()]

        elements = np.empty(anchor.shape[0], dtype=dtype_full)
        attributes = np.concatenate((anchor, normals, offset, anchor_feat, opacities, scale, rotation), axis=1)
        elements[:] = list(map(tuple, attributes))
        el = PlyElement.describe(elements, 'vertex')
        PlyData([el]).write(path)

    def save_ply_3dgs(self, path):
        mkdir_p(os.path.dirname(path))

        xyz = self._xyz.detach().cpu().numpy()
        normals = np.zeros_like(xyz)
        f_dc = self._features_dc.detach().transpose(1, 2).flatten(start_dim=1).contiguous().cpu().numpy()
        f_rest = self._features_rest.detach().transpose(1, 2).flatten(start_dim=1).contiguous().cpu().numpy()
        opacities = self._opacity.detach().cpu().numpy()
        scale = self._scaling.detach().cpu().numpy()
        rotation = self._rotation.detach().cpu().numpy()

        dtype_full = [(attribute, 'f4') for attribute in self.construct_list_of_attributes_3dgs()]

        elements = np.empty(xyz.shape[0], dtype=dtype_full)
        attributes = np.concatenate((xyz, normals, f_dc, f_rest, opacities, scale, rotation), axis=1)
        elements[:] = list(map(tuple, attributes))
        el = PlyElement.describe(elements, 'vertex')
        PlyData([el]).write(path)

    def load_ply_sparse_gaussian(self, path):
        plydata = PlyData.read(path)

        anchor = np.stack((np.asarray(plydata.elements[0]["x"]),
                        np.asarray(plydata.elements[0]["y"]),
                        np.asarray(plydata.elements[0]["z"])),  axis=1).astype(np.float32)
        opacities = np.asarray(plydata.elements[0]["opacity"])[..., np.newaxis].astype(np.float32)

        scale_names = [p.name for p in plydata.elements[0].properties if p.name.startswith("scale_")]
        scale_names = sorted(scale_names, key = lambda x: int(x.split('_')[-1]))
        scales = np.zeros((anchor.shape[0], len(scale_names)))
        for idx, attr_name in enumerate(scale_names):
            scales[:, idx] = np.asarray(plydata.elements[0][attr_name]).astype(np.float32)

        rot_names = [p.name for p in plydata.elements[0].properties if p.name.startswith("rot")]
        rot_names = sorted(rot_names, key = lambda x: int(x.split('_')[-1]))
        rots = np.zeros((anchor.shape[0], len(rot_names)))
        for idx, attr_name in enumerate(rot_names):
            rots[:, idx] = np.asarray(plydata.elements[0][attr_name]).astype(np.float32)
        
        # anchor_feat
        anchor_feat_names = [p.name for p in plydata.elements[0].properties if p.name.startswith("f_anchor_feat")]
        anchor_feat_names = sorted(anchor_feat_names, key = lambda x: int(x.split('_')[-1]))
        anchor_feats = np.zeros((anchor.shape[0], len(anchor_feat_names)))
        for idx, attr_name in enumerate(anchor_feat_names):
            anchor_feats[:, idx] = np.asarray(plydata.elements[0][attr_name]).astype(np.float32)

        offset_names = [p.name for p in plydata.elements[0].properties if p.name.startswith("f_offset")]
        offset_names = sorted(offset_names, key = lambda x: int(x.split('_')[-1]))
        offsets = np.zeros((anchor.shape[0], len(offset_names)))
        for idx, attr_name in enumerate(offset_names):
            offsets[:, idx] = np.asarray(plydata.elements[0][attr_name]).astype(np.float32)
        offsets = offsets.reshape((offsets.shape[0], 3, -1))
        
        self._anchor_feat = nn.Parameter(torch.tensor(anchor_feats, dtype=torch.float, device="cuda").requires_grad_(True))

        self._offset = nn.Parameter(torch.tensor(offsets, dtype=torch.float, device="cuda").transpose(1, 2).contiguous().requires_grad_(True))
        self._anchor = nn.Parameter(torch.tensor(anchor, dtype=torch.float, device="cuda").requires_grad_(True))
        self._opacity = nn.Parameter(torch.tensor(opacities, dtype=torch.float, device="cuda").requires_grad_(True))
        self._scaling = nn.Parameter(torch.tensor(scales, dtype=torch.float, device="cuda").requires_grad_(True))
        self._rotation = nn.Parameter(torch.tensor(rots, dtype=torch.float, device="cuda").requires_grad_(True))


    def replace_tensor_to_optimizer(self, tensor, name):
        optimizable_tensors = {}
        for group in self.optimizer.param_groups:
            if group["name"] == name:
                stored_state = self.optimizer.state.get(group['params'][0], None)
                stored_state["exp_avg"] = torch.zeros_like(tensor)
                stored_state["exp_avg_sq"] = torch.zeros_like(tensor)

                del self.optimizer.state[group['params'][0]]
                group["params"][0] = nn.Parameter(tensor.requires_grad_(True))
                self.optimizer.state[group['params'][0]] = stored_state

                optimizable_tensors[group["name"]] = group["params"][0]
        return optimizable_tensors


    def cat_tensors_to_optimizer(self, tensors_dict):
        optimizable_tensors = {}
        for group in self.optimizer.param_groups:
            if  'mlp' in group['name'] or \
                'conv' in group['name'] or \
                'feat_base' in group['name'] or \
                'embedding' in group['name']:
                continue
            assert len(group["params"]) == 1
            extension_tensor = tensors_dict[group["name"]]
            stored_state = self.optimizer.state.get(group['params'][0], None)
            if stored_state is not None:
                stored_state["exp_avg"] = torch.cat((stored_state["exp_avg"], torch.zeros_like(extension_tensor)), dim=0)
                stored_state["exp_avg_sq"] = torch.cat((stored_state["exp_avg_sq"], torch.zeros_like(extension_tensor)), dim=0)

                del self.optimizer.state[group['params'][0]]
                group["params"][0] = nn.Parameter(torch.cat((group["params"][0], extension_tensor), dim=0).requires_grad_(True))
                self.optimizer.state[group['params'][0]] = stored_state

                optimizable_tensors[group["name"]] = group["params"][0]
            else:
                group["params"][0] = nn.Parameter(torch.cat((group["params"][0], extension_tensor), dim=0).requires_grad_(True))
                optimizable_tensors[group["name"]] = group["params"][0]

        return optimizable_tensors


    # statis grad information to guide liftting. 
    def training_statis(self, viewspace_point_tensor, opacity, update_filter, offset_selection_mask, anchor_visible_mask):
        # update opacity stats
        temp_opacity = opacity.clone().view(-1).detach()
        temp_opacity[temp_opacity<0] = 0
        
        temp_opacity = temp_opacity.view([-1, self.n_offsets])
        self.opacity_accum[anchor_visible_mask] += temp_opacity.sum(dim=1, keepdim=True)
        
        # update anchor visiting statis
        self.anchor_demon[anchor_visible_mask] += 1

        # update neural gaussian statis
        anchor_visible_mask = anchor_visible_mask.unsqueeze(dim=1).repeat([1, self.n_offsets]).view(-1)
        combined_mask = torch.zeros_like(self.offset_gradient_accum, dtype=torch.bool).squeeze(dim=1)
        combined_mask[anchor_visible_mask] = offset_selection_mask
        temp_mask = combined_mask.clone()
        combined_mask[temp_mask] = update_filter
        
        grad_norm = torch.norm(viewspace_point_tensor.grad[update_filter,:2], dim=-1, keepdim=True)
        self.offset_gradient_accum[combined_mask] += grad_norm
        self.offset_denom[combined_mask] += 1

        

        
    def _prune_anchor_optimizer(self, mask):
        optimizable_tensors = {}
        for group in self.optimizer.param_groups:
            if  'mlp' in group['name'] or \
                'conv' in group['name'] or \
                'feat_base' in group['name'] or \
                'embedding' in group['name']:
                continue

            stored_state = self.optimizer.state.get(group['params'][0], None)
            if stored_state is not None:
                stored_state["exp_avg"] = stored_state["exp_avg"][mask]
                stored_state["exp_avg_sq"] = stored_state["exp_avg_sq"][mask]

                del self.optimizer.state[group['params'][0]]
                group["params"][0] = nn.Parameter((group["params"][0][mask].requires_grad_(True)))
                self.optimizer.state[group['params'][0]] = stored_state
                if group['name'] == "scaling":
                    scales = group["params"][0]
                    temp = scales[:,3:]
                    temp[temp>0.05] = 0.05
                    group["params"][0][:,3:] = temp
                optimizable_tensors[group["name"]] = group["params"][0]
            else:
                group["params"][0] = nn.Parameter(group["params"][0][mask].requires_grad_(True))
                if group['name'] == "scaling":
                    scales = group["params"][0]
                    temp = scales[:,3:]
                    temp[temp>0.05] = 0.05
                    group["params"][0][:,3:] = temp
                optimizable_tensors[group["name"]] = group["params"][0]
            
            
        return optimizable_tensors

    def prune_anchor(self,mask):
        valid_points_mask = ~mask

        optimizable_tensors = self._prune_anchor_optimizer(valid_points_mask)

        self._anchor = optimizable_tensors["anchor"]
        self._offset = optimizable_tensors["offset"]
        self._anchor_feat = optimizable_tensors["anchor_feat"]
        self._opacity = optimizable_tensors["opacity"]
        self._scaling = optimizable_tensors["scaling"]
        self._rotation = optimizable_tensors["rotation"]

            
    
    def anchor_growing(self, grads, threshold, offset_mask):
        ## 
        init_length = self.get_anchor.shape[0]*self.n_offsets
        for i in range(self.update_depth):
            # update threshold
            cur_threshold = threshold*((self.update_hierachy_factor//2)**i)
            # mask from grad threshold
            candidate_mask = (grads >= cur_threshold)
            candidate_mask = torch.logical_and(candidate_mask, offset_mask)
            
            # random pick
            rand_mask = torch.rand_like(candidate_mask.float())>(0.5**(i+1))
            rand_mask = rand_mask.cuda()
            candidate_mask = torch.logical_and(candidate_mask, rand_mask)
            
            length_inc = self.get_anchor.shape[0]*self.n_offsets - init_length
            if length_inc == 0:
                if i > 0:
                    continue
            else:
                candidate_mask = torch.cat([candidate_mask, torch.zeros(length_inc, dtype=torch.bool, device='cuda')], dim=0)

            all_xyz = self.get_anchor.unsqueeze(dim=1) + self._offset * self.get_scaling[:,:3].unsqueeze(dim=1)
            
            # assert self.update_init_factor // (self.update_hierachy_factor**i) > 0
            # size_factor = min(self.update_init_factor // (self.update_hierachy_factor**i), 1)
            # size_factor = self.update_init_factor // (self.update_hierachy_factor**i)

            # cur_size = self.voxel_size*size_factor
            cur_size = self.voxel_size / (2 ** i)
            
            grid_coords = torch.round(self.get_anchor / cur_size).int()

            selected_xyz = all_xyz.view([-1, 3])[candidate_mask]
            selected_grid_coords = torch.round(selected_xyz / cur_size).int()

            selected_grid_coords_unique, inverse_indices = torch.unique(selected_grid_coords, return_inverse=True, dim=0)


            ## split data for reducing peak memory calling
            use_chunk = True
            if use_chunk:
                chunk_size = 4096
                max_iters = grid_coords.shape[0] // chunk_size + (1 if grid_coords.shape[0] % chunk_size != 0 else 0)
                remove_duplicates_list = []
                for i in range(max_iters):
                    cur_remove_duplicates = (selected_grid_coords_unique.unsqueeze(1) == grid_coords[i*chunk_size:(i+1)*chunk_size, :]).all(-1).any(-1).view(-1)
                    remove_duplicates_list.append(cur_remove_duplicates)
                
                remove_duplicates = reduce(torch.logical_or, remove_duplicates_list)
            else:
                remove_duplicates = (selected_grid_coords_unique.unsqueeze(1) == grid_coords).all(-1).any(-1).view(-1)

            remove_duplicates = ~remove_duplicates
            candidate_anchor = selected_grid_coords_unique[remove_duplicates]*cur_size

            
            if candidate_anchor.shape[0] > 0:
                new_scaling = torch.ones_like(candidate_anchor).repeat([1,2]).float().cuda()*cur_size # *0.05
                new_scaling = torch.log(new_scaling)
                new_rotation = torch.zeros([candidate_anchor.shape[0], 4], device=candidate_anchor.device).float()
                new_rotation[:,0] = 1.0

                new_opacities = inverse_sigmoid(0.1 * torch.ones((candidate_anchor.shape[0], 1), dtype=torch.float, device="cuda"))

                new_feat = self._anchor_feat.unsqueeze(dim=1).repeat([1, self.n_offsets, 1]).view([-1, self.feat_dim])[candidate_mask]

                new_feat = scatter_max(new_feat, inverse_indices.unsqueeze(1).expand(-1, new_feat.size(1)), dim=0)[0][remove_duplicates]

                new_offsets = torch.zeros_like(candidate_anchor).unsqueeze(dim=1).repeat([1,self.n_offsets,1]).float().cuda()

                d = {
                    "anchor": candidate_anchor,
                    "scaling": new_scaling,
                    "rotation": new_rotation,
                    "anchor_feat": new_feat,
                    "offset": new_offsets,
                    "opacity": new_opacities,
                }
                

                temp_anchor_demon = torch.cat([self.anchor_demon, torch.zeros([new_opacities.shape[0], 1], device='cuda').float()], dim=0)
                del self.anchor_demon
                self.anchor_demon = temp_anchor_demon

                temp_opacity_accum = torch.cat([self.opacity_accum, torch.zeros([new_opacities.shape[0], 1], device='cuda').float()], dim=0)
                del self.opacity_accum
                self.opacity_accum = temp_opacity_accum

                torch.cuda.empty_cache()
                
                optimizable_tensors = self.cat_tensors_to_optimizer(d)
                self._anchor = optimizable_tensors["anchor"]
                self._scaling = optimizable_tensors["scaling"]
                self._rotation = optimizable_tensors["rotation"]
                self._anchor_feat = optimizable_tensors["anchor_feat"]
                self._offset = optimizable_tensors["offset"]
                self._opacity = optimizable_tensors["opacity"]

    
    def adjust_anchor_song(self, check_interval=100, success_threshold=0.8, grad_threshold=0.0002, min_opacity=0.005, require_purning=True, mu=0.3):
        # # adding anchors
        grads = self.offset_gradient_accum / self.offset_denom  # [N*k, 1]
        grads[grads.isnan()] = 0.0
        grads_norm = torch.norm(grads, dim=-1)
        offset_mask = (self.offset_denom > check_interval * success_threshold * 0.5).squeeze(dim=1)
        
        self.anchor_growing(grads_norm, grad_threshold, offset_mask)

        # update offset_denom
        self.offset_denom[offset_mask] = 0
        padding_offset_denom = torch.zeros([self.get_anchor.shape[0] * self.n_offsets - self.offset_denom.shape[0], 1],
                                        dtype=torch.int32, device=self.offset_denom.device)
        self.offset_denom = torch.cat([self.offset_denom, padding_offset_denom], dim=0)

        self.offset_gradient_accum[offset_mask] = 0
        padding_offset_gradient_accum = torch.zeros([self.get_anchor.shape[0] * self.n_offsets - self.offset_gradient_accum.shape[0], 1],
                                                    dtype=torch.int32, device=self.offset_gradient_accum.device)
        self.offset_gradient_accum = torch.cat([self.offset_gradient_accum, padding_offset_gradient_accum], dim=0)

        if require_purning:
            # ---- 1) opacity 기준 1차 pruning ----
            prune_mask_opacity = (self.opacity_accum < min_opacity * self.anchor_demon).squeeze(dim=1)
            anchors_mask = (self.anchor_demon > check_interval * success_threshold).squeeze(dim=1)
            prune_mask_opacity = torch.logical_and(prune_mask_opacity, anchors_mask)

            # ---- 2) scale 기반 보정 ----
            scales = torch.exp(self._scaling[:, :3])
            num_anchors = self.get_anchor.shape[0]
            expected_len = num_anchors * self.n_offsets

            if scales.shape[0] > expected_len:
                scales = scales[:expected_len, :]
            elif scales.shape[0] < expected_len:
                pad_len = expected_len - scales.shape[0]
                pad = torch.ones((pad_len, 3), device=scales.device, dtype=scales.dtype)
                scales = torch.cat([scales, pad], dim=0)

            if self.n_offsets > 1:
                scales_anchor = scales.view(num_anchors, self.n_offsets, 3).mean(dim=1)
            else:
                scales_anchor = scales

            U = torch.norm(scales_anchor, dim=1)
            U_median = torch.median(U)
            U_tilde = U / (U_median + 1e-8)

            lambda_s = mu
            scale_weight = torch.exp(-lambda_s * U_tilde)
            importance = self.opacity_accum.squeeze() * scale_weight

            prune_mask_scale = (importance < min_opacity * self.anchor_demon.squeeze())
            prune_mask_scale = torch.logical_and(prune_mask_scale, anchors_mask)

            # ---- 3) 최종 마스크 (scale 보정 포함) ----
            prune_mask_final = prune_mask_scale

            # ✅ LOGGING
            total_anchors = num_anchors
            pruned_opacity = int(prune_mask_opacity.sum().item())
            pruned_scale = int(prune_mask_scale.sum().item())
            additional_scale = pruned_scale - pruned_opacity if pruned_scale > pruned_opacity else 0
            kept = total_anchors - pruned_scale

            # mean_scale = float(scales_anchor.mean().item())
            # median_scale = float(U_median.item())
            # mean_importance = float(importance.mean().item())

            # print(
            #     f"[ScalePrune] total={total_anchors} | "
            #     f"opacity_pruned={pruned_opacity} | "
            #     f"scale_pruned={pruned_scale} (+{additional_scale}) | "
            #     f"kept={kept}"
            # )

            # ---- 4) pruning 실제 적용 ----
            prune_mask = prune_mask_final
            

            # update offset_denom
            offset_denom = self.offset_denom.view([-1, self.n_offsets])[~prune_mask]
            offset_denom = offset_denom.view([-1, 1])
            del self.offset_denom
            self.offset_denom = offset_denom

            offset_gradient_accum = self.offset_gradient_accum.view([-1, self.n_offsets])[~prune_mask]
            offset_gradient_accum = offset_gradient_accum.view([-1, 1])
            del self.offset_gradient_accum
            self.offset_gradient_accum = offset_gradient_accum
            
            # update opacity accum
            if anchors_mask.sum() > 0:
                self.opacity_accum[anchors_mask] = torch.zeros([anchors_mask.sum(), 1], device='cuda').float()
                self.anchor_demon[anchors_mask] = torch.zeros([anchors_mask.sum(), 1], device='cuda').float()

            temp_opacity_accum = self.opacity_accum[~prune_mask]
            del self.opacity_accum
            self.opacity_accum = temp_opacity_accum

            temp_anchor_demon = self.anchor_demon[~prune_mask]
            del self.anchor_demon
            self.anchor_demon = temp_anchor_demon

            if prune_mask.shape[0] > 0:
                self.prune_anchor(prune_mask)

        self.max_radii2D = torch.zeros((self.get_anchor.shape[0]), device="cuda")


    def adjust_anchor(self, check_interval=100, success_threshold=0.8, grad_threshold=0.0002, min_opacity=0.005, require_purning=True):
        # # adding anchors
        grads = self.offset_gradient_accum / self.offset_denom  # [N*k, 1]
        grads[grads.isnan()] = 0.0
        grads_norm = torch.norm(grads, dim=-1)
        offset_mask = (self.offset_denom > check_interval * success_threshold * 0.5).squeeze(dim=1)
        
        self.anchor_growing(grads_norm, grad_threshold, offset_mask)

        # update offset_denom
        self.offset_denom[offset_mask] = 0
        padding_offset_denom = torch.zeros([self.get_anchor.shape[0] * self.n_offsets - self.offset_denom.shape[0], 1],
                                        dtype=torch.int32, device=self.offset_denom.device)
        self.offset_denom = torch.cat([self.offset_denom, padding_offset_denom], dim=0)

        self.offset_gradient_accum[offset_mask] = 0
        padding_offset_gradient_accum = torch.zeros([self.get_anchor.shape[0] * self.n_offsets - self.offset_gradient_accum.shape[0], 1],
                                                    dtype=torch.int32, device=self.offset_gradient_accum.device)
        self.offset_gradient_accum = torch.cat([self.offset_gradient_accum, padding_offset_gradient_accum], dim=0)

        if require_purning:
            # # prune anchors
            prune_mask = (self.opacity_accum < min_opacity * self.anchor_demon).squeeze(dim=1)
            anchors_mask = (self.anchor_demon > check_interval * success_threshold).squeeze(dim=1)  # [N, 1]
            prune_mask = torch.logical_and(prune_mask, anchors_mask)  # [N] 
        
            # update offset_denom
            offset_denom = self.offset_denom.view([-1, self.n_offsets])[~prune_mask]
            offset_denom = offset_denom.view([-1, 1])
            del self.offset_denom
            self.offset_denom = offset_denom

            offset_gradient_accum = self.offset_gradient_accum.view([-1, self.n_offsets])[~prune_mask]
            offset_gradient_accum = offset_gradient_accum.view([-1, 1])
            del self.offset_gradient_accum
            self.offset_gradient_accum = offset_gradient_accum
            
            # update opacity accum
            if anchors_mask.sum() > 0:
                self.opacity_accum[anchors_mask] = torch.zeros([anchors_mask.sum(), 1], device='cuda').float()
                self.anchor_demon[anchors_mask] = torch.zeros([anchors_mask.sum(), 1], device='cuda').float()

            temp_opacity_accum = self.opacity_accum[~prune_mask]
            del self.opacity_accum
            self.opacity_accum = temp_opacity_accum

            temp_anchor_demon = self.anchor_demon[~prune_mask]
            del self.anchor_demon
            self.anchor_demon = temp_anchor_demon

            if prune_mask.shape[0] > 0:
                self.prune_anchor(prune_mask)

        self.max_radii2D = torch.zeros((self.get_anchor.shape[0]), device="cuda")


    def adjust_anchor_heejung(
        self,
        check_interval=100,
        success_threshold=0.8,
        grad_threshold=0.0002,
        min_opacity=0.005,
        require_purning=True,
        frame_trust=1.0,   ### 🌟 추가됨
        bit_trust=0.0,     ### 🌟 추가됨
        debug=False        ### 🌟 추가됨
    ):
        # =========================================================
        # 🔹 1. grad_threshold 동적 조정 (비선형 역비례식)
        # =========================================================
        dynamic_grad_threshold = grad_threshold / (1.0 + bit_trust + frame_trust)   ### 🌟 추가됨
        dynamic_grad_threshold = max(dynamic_grad_threshold, grad_threshold * 0.3)  ### 🌟 추가됨

        if debug:  ### 🌟 추가됨
            print(f"[Adjust Anchor] bit={bit_trust:.3f}, frame={frame_trust:.3f} "
                f"→ grad_th {grad_threshold:.5f} → {dynamic_grad_threshold:.5f}")

        # =========================================================
        # 기존 anchor_growing 로직 (grad_threshold만 수정)
        # =========================================================
        grads = self.offset_gradient_accum / self.offset_denom
        grads[grads.isnan()] = 0.0
        grads_norm = torch.norm(grads, dim=-1)
        offset_mask = (self.offset_denom > check_interval * success_threshold * 0.5).squeeze(dim=1)

        self.anchor_growing(grads_norm, dynamic_grad_threshold, offset_mask)  ### 🌟 grad_threshold → dynamic_grad_threshold

        # =========================================================
        # 이후 부분: 원본 코드 동일
        # =========================================================
        self.offset_denom[offset_mask] = 0
        padding_offset_denom = torch.zeros(
            [self.get_anchor.shape[0] * self.n_offsets - self.offset_denom.shape[0], 1],
            dtype=torch.int32, device=self.offset_denom.device
        )
        self.offset_denom = torch.cat([self.offset_denom, padding_offset_denom], dim=0)

        self.offset_gradient_accum[offset_mask] = 0
        padding_offset_gradient_accum = torch.zeros(
            [self.get_anchor.shape[0] * self.n_offsets - self.offset_gradient_accum.shape[0], 1],
            dtype=torch.int32, device=self.offset_gradient_accum.device
        )
        self.offset_gradient_accum = torch.cat([self.offset_gradient_accum, padding_offset_gradient_accum], dim=0)

        if require_purning:
            prune_mask = (self.opacity_accum < min_opacity * self.anchor_demon).squeeze(dim=1)  ### 🌟 opacity는 기존 값 유지
            anchors_mask = (self.anchor_demon > check_interval * success_threshold).squeeze(dim=1)
            prune_mask = torch.logical_and(prune_mask, anchors_mask)
        
            offset_denom = self.offset_denom.view([-1, self.n_offsets])[~prune_mask]
            offset_denom = offset_denom.view([-1, 1])
            del self.offset_denom
            self.offset_denom = offset_denom

            offset_gradient_accum = self.offset_gradient_accum.view([-1, self.n_offsets])[~prune_mask]
            offset_gradient_accum = offset_gradient_accum.view([-1, 1])
            del self.offset_gradient_accum
            self.offset_gradient_accum = offset_gradient_accum
            
            if anchors_mask.sum() > 0:
                self.opacity_accum[anchors_mask] = torch.zeros([anchors_mask.sum(), 1], device='cuda').float()
                self.anchor_demon[anchors_mask] = torch.zeros([anchors_mask.sum(), 1], device='cuda').float()

            temp_opacity_accum = self.opacity_accum[~prune_mask]
            del self.opacity_accum
            self.opacity_accum = temp_opacity_accum

            temp_anchor_demon = self.anchor_demon[~prune_mask]
            del self.anchor_demon
            self.anchor_demon = temp_anchor_demon

            if prune_mask.shape[0] > 0:
                self.prune_anchor(prune_mask)

        self.max_radii2D = torch.zeros((self.get_anchor.shape[0]), device="cuda")

        if debug:  ### 🌟 추가됨
            print(f"[Adjust Anchor Done] Grad_Th={dynamic_grad_threshold:.6f}")


    # def adjust_anchor_heejung (self,
    #                 check_interval=100,
    #                 success_threshold=0.8,
    #                 grad_threshold=0.0002,
    #                 min_opacity=0.005,
    #                 require_purning=True,
    #                 frame_trust=1.0):  ### 🌟 수정됨: frame_trust 인자 추가
    #     """
    #     frame_trust: [0.0 ~ 1.0] 사이 값 (frame_trust_dict[frame_id])
    #                     높을수록 중요한 프레임 → 더 많은 growing, 덜 pruning
    #     """

    #     dynamic_grad_threshold = grad_threshold * (1.1 - 0.2 * frame_trust)
    #     dynamic_min_opacity = min_opacity * (0.9 + 0.2 * frame_trust)

    #     # if frame_trust < 0.5:  ### 🌟 수정됨: frame_trust 로그 추가
    #     #     print(f"[Adjust Anchor] Low-trust frame ({frame_trust:.2f}) → grad_th ↑ {dynamic_grad_threshold:.5f}, min_op ↓ {dynamic_min_opacity:.5f}")
    #     # else:
    #     #     print(f"[Adjust Anchor] High-trust frame ({frame_trust:.2f}) → grad_th ↓ {dynamic_grad_threshold:.5f}, min_op ↑ {dynamic_min_opacity:.5f}")

    #     grads = self.offset_gradient_accum / self.offset_denom
    #     grads[grads.isnan()] = 0.0
    #     grads_norm = torch.norm(grads, dim=-1)
    #     offset_mask = (self.offset_denom > check_interval * success_threshold * 0.5).squeeze(dim=1)

    #     self.anchor_growing(grads_norm, dynamic_grad_threshold, offset_mask)  ### 🌟 수정됨: grad_threshold → dynamic_grad_threshold

    #     self.offset_denom[offset_mask] = 0
    #     padding_offset_denom = torch.zeros(
    #         [self.get_anchor.shape[0] * self.n_offsets - self.offset_denom.shape[0], 1],
    #         dtype=torch.int32, device=self.offset_denom.device
    #     )
    #     self.offset_denom = torch.cat([self.offset_denom, padding_offset_denom], dim=0)

    #     self.offset_gradient_accum[offset_mask] = 0
    #     padding_offset_gradient_accum = torch.zeros(
    #         [self.get_anchor.shape[0] * self.n_offsets - self.offset_gradient_accum.shape[0], 1],
    #         dtype=torch.int32, device=self.offset_gradient_accum.device
    #     )
    #     self.offset_gradient_accum = torch.cat([self.offset_gradient_accum, padding_offset_gradient_accum], dim=0)

    #     if require_purning:
    #         prune_mask = (self.opacity_accum < dynamic_min_opacity * self.anchor_demon).squeeze(dim=1)  ### 🌟 수정됨: min_opacity → dynamic_min_opacity
    #         anchors_mask = (self.anchor_demon > check_interval * success_threshold).squeeze(dim=1)
    #         prune_mask = torch.logical_and(prune_mask, anchors_mask)

    #         offset_denom = self.offset_denom.view([-1, self.n_offsets])[~prune_mask]
    #         offset_denom = offset_denom.view([-1, 1])
    #         self.offset_denom = offset_denom

    #         offset_gradient_accum = self.offset_gradient_accum.view([-1, self.n_offsets])[~prune_mask]
    #         offset_gradient_accum = offset_gradient_accum.view([-1, 1])
    #         self.offset_gradient_accum = offset_gradient_accum

    #         if anchors_mask.sum() > 0:
    #             self.opacity_accum[anchors_mask] = torch.zeros([anchors_mask.sum(), 1], device='cuda').float()
    #             self.anchor_demon[anchors_mask] = torch.zeros([anchors_mask.sum(), 1], device='cuda').float()

    #         temp_opacity_accum = self.opacity_accum[~prune_mask]
    #         self.opacity_accum = temp_opacity_accum

    #         temp_anchor_demon = self.anchor_demon[~prune_mask]
    #         self.anchor_demon = temp_anchor_demon

    #         if prune_mask.shape[0] > 0:
    #             self.prune_anchor(prune_mask)

    #     self.max_radii2D = torch.zeros((self.get_anchor.shape[0]), device="cuda")



    @torch.no_grad()
    def densify_occlusion(self, view, depth, opacity_mask):
        H, W = depth.shape
        v, u = torch.meshgrid(torch.arange(H, device=depth.device), torch.arange(W, device=depth.device))
        z = depth
        x = (u - W * 0.5) * z / view.intrinsic[0, 0]
        y = (v - H * 0.5) * z / view.intrinsic[1, 1]
        xyz = torch.stack([x, y, z], dim=0).reshape(3, -1).T
        edge_mask = depth_edge(depth, rtol=0.03).reshape(-1)
        
        valid_mask = torch.logical_and(opacity_mask, ~edge_mask)
        xyz = geom_transform_points(xyz, view.view_world_transform)[valid_mask]
        xyz = xyz[::self.ratio]

        if xyz.shape[0] == 0:
            return

        candidate_anchor, voxel_sizes = self.octree_sample(xyz)

        def compute_hash(points, voxel_size):
            hash_values = points[:, 0] * 73856093 + points[:, 1] * 19349663 + points[:, 2] * 83492791
            return hash_values

        existing_hash = compute_hash(self._anchor, self.voxel_size)
        existing_hash_set = set(existing_hash.cpu().numpy())

        candidate_hash = compute_hash(candidate_anchor, self.voxel_size)
        
        unique_mask = torch.tensor([h not in existing_hash_set for h in candidate_hash.cpu().numpy()], 
                                 device=candidate_anchor.device, dtype=torch.bool)
        
        candidate_anchor = candidate_anchor[unique_mask]
        voxel_sizes = voxel_sizes[unique_mask]

        if candidate_anchor.shape[0] == 0:
            return

        dist2 = torch.ones_like(candidate_anchor[:, :1]) * voxel_sizes.unsqueeze(-1)
        new_scaling = torch.log(torch.sqrt(dist2)).repeat(1, 6)
  
        new_rotation = torch.zeros([candidate_anchor.shape[0], 4], device=candidate_anchor.device).float()
        new_rotation[:,0] = 1.0

        new_opacities = inverse_sigmoid(0.1 * torch.ones((candidate_anchor.shape[0], 1), dtype=torch.float, device="cuda"))

        new_feat = torch.zeros([candidate_anchor.shape[0], self.feat_dim], device=candidate_anchor.device).float()

        new_offsets = torch.zeros_like(candidate_anchor).unsqueeze(dim=1).repeat([1,self.n_offsets,1]).float().cuda()

        d = {
            "anchor": candidate_anchor,
            "scaling": new_scaling,
            "rotation": new_rotation,
            "anchor_feat": new_feat,
            "offset": new_offsets,
            "opacity": new_opacities,
        }
        

        temp_anchor_demon = torch.cat([self.anchor_demon, torch.zeros([new_opacities.shape[0], 1], device='cuda').float()], dim=0)
        del self.anchor_demon
        self.anchor_demon = temp_anchor_demon

        temp_opacity_accum = torch.cat([self.opacity_accum, torch.zeros([new_opacities.shape[0], 1], device='cuda').float()], dim=0)
        del self.opacity_accum
        self.opacity_accum = temp_opacity_accum

        torch.cuda.empty_cache()
        
        optimizable_tensors = self.cat_tensors_to_optimizer(d)
        self._anchor = optimizable_tensors["anchor"]
        self._scaling = optimizable_tensors["scaling"]
        self._rotation = optimizable_tensors["rotation"]
        self._anchor_feat = optimizable_tensors["anchor_feat"]
        self._offset = optimizable_tensors["offset"]
        self._opacity = optimizable_tensors["opacity"]

        # update offset_denom
        padding_offset_demon = torch.zeros([self.get_anchor.shape[0]*self.n_offsets - self.offset_denom.shape[0], 1],
                                           dtype=torch.int32, 
                                           device=self.offset_denom.device)
        self.offset_denom = torch.cat([self.offset_denom, padding_offset_demon], dim=0)

        padding_offset_gradient_accum = torch.zeros([self.get_anchor.shape[0]*self.n_offsets - self.offset_gradient_accum.shape[0], 1],
                                           dtype=torch.int32, 
                                           device=self.offset_gradient_accum.device)
        self.offset_gradient_accum = torch.cat([self.offset_gradient_accum, padding_offset_gradient_accum], dim=0)

        self.max_radii2D = torch.zeros((self.get_anchor.shape[0]), device="cuda")

    def prune_gaussians(self, percent, import_score: list):
        sorted_tensor, _ = torch.sort(import_score, dim=0)
        index_nth_percentile = int(percent * (sorted_tensor.shape[0] - 1))
        value_nth_percentile = sorted_tensor[index_nth_percentile]
        prune_mask = (import_score <= value_nth_percentile).squeeze()
        self.prune_anchor(prune_mask)
        


    def save_mlp_checkpoints(self, path, mode = 'split'):#split or unite
        mkdir_p(os.path.dirname(path))
        if mode == 'split':
            self.mlp_opacity.eval()
            opacity_mlp = torch.jit.trace(self.mlp_opacity, (torch.rand(1, self.feat_dim+3+self.opacity_dist_dim).cuda()))
            opacity_mlp.save(os.path.join(path, 'opacity_mlp.pt'))
            self.mlp_opacity.train()

            self.mlp_cov.eval()
            cov_mlp = torch.jit.trace(self.mlp_cov, (torch.rand(1, self.feat_dim+3+self.cov_dist_dim).cuda()))
            cov_mlp.save(os.path.join(path, 'cov_mlp.pt'))
            self.mlp_cov.train()

            self.mlp_color.eval()
            color_mlp = torch.jit.trace(self.mlp_color, (torch.rand(1, self.feat_dim+3+self.color_dist_dim+self.appearance_dim).cuda()))
            color_mlp.save(os.path.join(path, 'color_mlp.pt'))
            self.mlp_color.train()

            if self.use_feat_bank:
                self.mlp_feature_bank.eval()
                feature_bank_mlp = torch.jit.trace(self.mlp_feature_bank, (torch.rand(1, 3+1).cuda()))
                feature_bank_mlp.save(os.path.join(path, 'feature_bank_mlp.pt'))
                self.mlp_feature_bank.train()

            if self.appearance_dim:
                self.embedding_appearance.eval()
                emd = torch.jit.trace(self.embedding_appearance, (torch.zeros((1,), dtype=torch.long).cuda()))
                emd.save(os.path.join(path, 'embedding_appearance.pt'))
                self.embedding_appearance.train()

        elif mode == 'unite':
            if self.use_feat_bank:
                torch.save({
                    'opacity_mlp': self.mlp_opacity.state_dict(),
                    'cov_mlp': self.mlp_cov.state_dict(),
                    'color_mlp': self.mlp_color.state_dict(),
                    'feature_bank_mlp': self.mlp_feature_bank.state_dict(),
                    'appearance': self.embedding_appearance.state_dict()
                    }, os.path.join(path, 'checkpoints.pth'))
            elif self.appearance_dim > 0:
                torch.save({
                    'opacity_mlp': self.mlp_opacity.state_dict(),
                    'cov_mlp': self.mlp_cov.state_dict(),
                    'color_mlp': self.mlp_color.state_dict(),
                    'appearance': self.embedding_appearance.state_dict()
                    }, os.path.join(path, 'checkpoints.pth'))
            else:
                torch.save({
                    'opacity_mlp': self.mlp_opacity.state_dict(),
                    'cov_mlp': self.mlp_cov.state_dict(),
                    'color_mlp': self.mlp_color.state_dict(),
                    }, os.path.join(path, 'checkpoints.pth'))
        else:
            raise NotImplementedError


    def load_mlp_checkpoints(self, path, mode = 'split'):#split or unite
        if mode == 'split':
            self.mlp_opacity = torch.jit.load(os.path.join(path, 'opacity_mlp.pt')).cuda()
            self.mlp_cov = torch.jit.load(os.path.join(path, 'cov_mlp.pt')).cuda()
            self.mlp_color = torch.jit.load(os.path.join(path, 'color_mlp.pt')).cuda()
            if self.use_feat_bank:
                self.mlp_feature_bank = torch.jit.load(os.path.join(path, 'feature_bank_mlp.pt')).cuda()
            if self.appearance_dim > 0:
                self.embedding_appearance = torch.jit.load(os.path.join(path, 'embedding_appearance.pt')).cuda()
        elif mode == 'unite':
            checkpoint = torch.load(os.path.join(path, 'checkpoints.pth'))
            self.mlp_opacity.load_state_dict(checkpoint['opacity_mlp'])
            self.mlp_cov.load_state_dict(checkpoint['cov_mlp'])
            self.mlp_color.load_state_dict(checkpoint['color_mlp'])
            if self.use_feat_bank:
                self.mlp_feature_bank.load_state_dict(checkpoint['feature_bank_mlp'])
            if self.appearance_dim > 0:
                self.embedding_appearance.load_state_dict(checkpoint['appearance'])
        else:
            raise NotImplementedError


    # 새로 추가
    def append_importance(self, num_new, val):
        new_vals = torch.full((num_new, 1), val, device="cuda")
        self.importance = torch.cat([self.importance, new_vals], dim=0)